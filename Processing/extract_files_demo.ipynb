{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca557ec9",
   "metadata": {},
   "source": [
    "# Excel Data Extraction and Cleaning for ML Models\n",
    "\n",
    "This notebook demonstrates how the `extract_files.py` script loads Excel files and cleans them for use in ML inference.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The `extract_files.py` script provides functionality to:\n",
    "1. Read Excel files (.xlsx/.xls) with multiple engine fallbacks\n",
    "2. Select specific columns by name or index\n",
    "3. Handle missing values (drop or fill)\n",
    "4. Convert to NumPy arrays\n",
    "5. Apply optional scaling (standard or minmax)\n",
    "6. Save processed data as .npz files with metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba102d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add the Processing directory to the path\n",
    "sys.path.insert(0, os.path.dirname(os.path.abspath('__file__')))\n",
    "\n",
    "# Reload the module to get the latest changes\n",
    "import importlib\n",
    "if 'extract_files' in sys.modules:\n",
    "    importlib.reload(sys.modules['extract_files'])\n",
    "\n",
    "# Import functions from extract_files\n",
    "from extract_files import (\n",
    "    _read_excel_with_fallbacks,\n",
    "    _fix_excel_headers,\n",
    "    _select_columns,\n",
    "    _handle_missing,\n",
    "    _scale_array,\n",
    "    excel_to_numpy\n",
    ")\n",
    "\n",
    "print(\"✅ Module loaded with header fix!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc385c6",
   "metadata": {},
   "source": [
    "## Step 1: Set the File Path\n",
    "\n",
    "For this example, we'll use a placeholder filename. Replace this with an actual file path from your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21ff72e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target file: ../Data/Processed Data/Extract Data 0.6kV Kinetrics/K-016084-677a.xls\n"
     ]
    }
   ],
   "source": [
    "# Set the filename - replace with actual file path\n",
    "# FILENAME = \"../Data/Processed Data/Extract Data 480V/KINETRICS/08-5103VI.xlsx\"\n",
    "\n",
    "# For demonstration, let's use an actual file from the project\n",
    "# Uncomment one of these lines to use real data:\n",
    "FILENAME = \"../Data/Processed Data/Extract Data 0.6kV Kinetrics/K-016084-677a.xls\"\n",
    "# FILENAME = \"../Data/Processed Data/Extract Data 480V/some_file.xls\"\n",
    "\n",
    "print(f\"Target file: {FILENAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e360b1f0",
   "metadata": {},
   "source": [
    "## Step 2: Read Excel File\n",
    "\n",
    "The `_read_excel_with_fallbacks` function tries multiple engines to read the Excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455327cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read 3 sheet(s)\n",
      "Sheet names: ['Sheet1', 'Sheet2', 'Sheet3']\n",
      "\n",
      "============================================================\n",
      "FIXING HEADERS\n",
      "============================================================\n",
      "\n",
      "--- Sheet1 ---\n",
      "Before fix - Columns: ['Recording title: ', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']\n",
      "Before fix - Shape: (10005, 10)\n",
      "Error reading file: name 'fix_excel_headers' is not defined\n",
      "\n",
      "Please update FILENAME with a valid Excel file path.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/_6/sh39ykv94sz7qqnm5qywwbzm0000gn/T/ipykernel_45000/3438610182.py\", line 19, in <module>\n",
      "    df_fixed = fix_excel_headers(df)\n",
      "               ^^^^^^^^^^^^^^^^^\n",
      "NameError: name 'fix_excel_headers' is not defined\n"
     ]
    }
   ],
   "source": [
    "# Read the Excel file\n",
    "# This will return a dictionary of {sheet_name: DataFrame}\n",
    "try:\n",
    "    sheets_dict = _read_excel_with_fallbacks(FILENAME, sheet=None)\n",
    "    print(f\"Successfully read {len(sheets_dict)} sheet(s)\")\n",
    "    print(f\"Sheet names: {list(sheets_dict.keys())}\")\n",
    "    \n",
    "    # Apply header fix to each sheet\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FIXING HEADERS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    sheets_dict_fixed = {}\n",
    "    for sheet_name, df in sheets_dict.items():\n",
    "        print(f\"\\n--- {sheet_name} ---\")\n",
    "        print(f\"Before fix - Columns: {list(df.columns[:5])}\")\n",
    "        print(f\"Before fix - Shape: {df.shape}\")\n",
    "        \n",
    "        df_fixed = _fix_excel_headers(df)\n",
    "        sheets_dict_fixed[sheet_name] = df_fixed\n",
    "        \n",
    "        print(f\"After fix - Columns: {list(df_fixed.columns[:5])}\")\n",
    "        print(f\"After fix - Shape: {df_fixed.shape}\")\n",
    "        \n",
    "        if not df_fixed.empty:\n",
    "            print(f\"\\nFirst few rows with CORRECT column names:\")\n",
    "            display(df_fixed.head())\n",
    "    \n",
    "    # Update the main dictionary\n",
    "    sheets_dict = sheets_dict_fixed\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error reading file: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    print(\"\\nPlease update FILENAME with a valid Excel file path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138493a8",
   "metadata": {},
   "source": [
    "## Step 3: Select and Clean Data\n",
    "\n",
    "Let's work with the first sheet and demonstrate various cleaning operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "290e871b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with sheet: Sheet1\n",
      "Original shape: (10005, 10)\n",
      "\n",
      "Data types:\n",
      "Recording title:     object\n",
      "Unnamed: 1           object\n",
      "Unnamed: 2           object\n",
      "Unnamed: 3           object\n",
      "Unnamed: 4           object\n",
      "Unnamed: 5           object\n",
      "Unnamed: 6           object\n",
      "Unnamed: 7           object\n",
      "Unnamed: 8           object\n",
      "Unnamed: 9           object\n",
      "dtype: object\n",
      "\n",
      "Missing values:\n",
      "Recording title:     0\n",
      "Unnamed: 1           0\n",
      "Unnamed: 2           3\n",
      "Unnamed: 3           3\n",
      "Unnamed: 4           3\n",
      "Unnamed: 5           3\n",
      "Unnamed: 6           3\n",
      "Unnamed: 7           3\n",
      "Unnamed: 8           3\n",
      "Unnamed: 9           3\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Select the first sheet\n",
    "sheet_name = list(sheets_dict.keys())[0]\n",
    "df = sheets_dict[sheet_name].copy()\n",
    "\n",
    "print(f\"Working with sheet: {sheet_name}\")\n",
    "print(f\"Original shape: {df.shape}\")\n",
    "print(f\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "print(f\"\\nMissing values:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18b994f",
   "metadata": {},
   "source": [
    "### Step 3.1: Select Only Numeric Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebdb312b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After selecting numeric columns: (10005, 0)\n",
      "Numeric columns: []\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: [0, 1, 2, 3, 4]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Select only numeric columns\n",
    "df_numeric = _select_columns(df, columns=None, numeric_only=True)\n",
    "\n",
    "print(f\"After selecting numeric columns: {df_numeric.shape}\")\n",
    "print(f\"Numeric columns: {list(df_numeric.columns)}\")\n",
    "display(df_numeric.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457bb496",
   "metadata": {},
   "source": [
    "### Step 3.2: Handle Missing Values\n",
    "\n",
    "We can either drop rows with missing values or fill them with a specific value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56dbda4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After dropping NaN rows: (10005, 0)\n",
      "After filling NaN with 0: (10005, 0)\n"
     ]
    }
   ],
   "source": [
    "# Option 1: Drop rows with any missing values\n",
    "df_no_na = _handle_missing(df_numeric, dropna=True, fillna=None)\n",
    "print(f\"After dropping NaN rows: {df_no_na.shape}\")\n",
    "\n",
    "# Option 2: Fill missing values with 0 (or any other value)\n",
    "df_filled = _handle_missing(df_numeric, dropna=False, fillna=0.0)\n",
    "print(f\"After filling NaN with 0: {df_filled.shape}\")\n",
    "\n",
    "# Use the no-NA version for further processing\n",
    "df_clean = df_no_na.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa57f16b",
   "metadata": {},
   "source": [
    "## Step 4: Convert to NumPy Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2906ed5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy array shape: (10005, 0)\n",
      "Data type: float64\n",
      "\n",
      "Array statistics:\n",
      "Min: []\n",
      "Max: []\n",
      "Mean: []\n",
      "Std: []\n"
     ]
    }
   ],
   "source": [
    "# Convert to NumPy array\n",
    "array_unscaled = df_clean.to_numpy()\n",
    "\n",
    "print(f\"NumPy array shape: {array_unscaled.shape}\")\n",
    "print(f\"Data type: {array_unscaled.dtype}\")\n",
    "print(f\"\\nArray statistics:\")\n",
    "print(f\"Min: {np.min(array_unscaled, axis=0)}\")\n",
    "print(f\"Max: {np.max(array_unscaled, axis=0)}\")\n",
    "print(f\"Mean: {np.mean(array_unscaled, axis=0)}\")\n",
    "print(f\"Std: {np.std(array_unscaled, axis=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04460958",
   "metadata": {},
   "source": [
    "## Step 5: Apply Scaling\n",
    "\n",
    "Scaling is important for ML models. We can apply:\n",
    "- **Standard scaling**: (x - mean) / std\n",
    "- **MinMax scaling**: (x - min) / (max - min)\n",
    "- **None**: No scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519373c4",
   "metadata": {},
   "source": [
    "### Standard Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6bad487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Scaling Applied\n",
      "Scaled array shape: (10005, 0)\n",
      "\n",
      "Scaling parameters (mean, std):\n",
      "\n",
      "Scaled array statistics:\n",
      "Mean (should be ~0): []\n",
      "Std (should be ~1): []\n"
     ]
    }
   ],
   "source": [
    "# Apply standard scaling\n",
    "array_standard, params_standard = _scale_array(array_unscaled, mode=\"standard\")\n",
    "\n",
    "print(\"Standard Scaling Applied\")\n",
    "print(f\"Scaled array shape: {array_standard.shape}\")\n",
    "print(f\"\\nScaling parameters (mean, std):\")\n",
    "for col_idx, (mean, std) in params_standard.items():\n",
    "    col_name = df_clean.columns[int(col_idx)] if int(col_idx) < len(df_clean.columns) else f\"Col_{col_idx}\"\n",
    "    print(f\"  {col_name}: mean={mean:.4f}, std={std:.4f}\")\n",
    "\n",
    "print(f\"\\nScaled array statistics:\")\n",
    "print(f\"Mean (should be ~0): {np.mean(array_standard, axis=0)}\")\n",
    "print(f\"Std (should be ~1): {np.std(array_standard, axis=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847c034c",
   "metadata": {},
   "source": [
    "### MinMax Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac5ac75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMax Scaling Applied\n",
      "Scaled array shape: (10005, 0)\n",
      "\n",
      "Scaling parameters (min, max):\n",
      "\n",
      "Scaled array statistics:\n",
      "Min (should be 0): []\n",
      "Max (should be 1): []\n"
     ]
    }
   ],
   "source": [
    "# Apply minmax scaling\n",
    "array_minmax, params_minmax = _scale_array(array_unscaled, mode=\"minmax\")\n",
    "\n",
    "print(\"MinMax Scaling Applied\")\n",
    "print(f\"Scaled array shape: {array_minmax.shape}\")\n",
    "print(f\"\\nScaling parameters (min, max):\")\n",
    "for col_idx, (min_val, max_val) in params_minmax.items():\n",
    "    col_name = df_clean.columns[int(col_idx)] if int(col_idx) < len(df_clean.columns) else f\"Col_{col_idx}\"\n",
    "    print(f\"  {col_name}: min={min_val:.4f}, max={max_val:.4f}\")\n",
    "\n",
    "print(f\"\\nScaled array statistics:\")\n",
    "print(f\"Min (should be 0): {np.min(array_minmax, axis=0)}\")\n",
    "print(f\"Max (should be 1): {np.max(array_minmax, axis=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021ab2cc",
   "metadata": {},
   "source": [
    "## Step 6: Visualize Scaling Effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "880011c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the effect of scaling on the first column\n",
    "if array_unscaled.shape[1] > 0:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # Original data\n",
    "    axes[0].hist(array_unscaled[:, 0], bins=20, edgecolor='black')\n",
    "    axes[0].set_title('Original Data (First Column)')\n",
    "    axes[0].set_xlabel('Value')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    \n",
    "    # Standard scaled\n",
    "    axes[1].hist(array_standard[:, 0], bins=20, edgecolor='black', color='orange')\n",
    "    axes[1].set_title('Standard Scaled')\n",
    "    axes[1].set_xlabel('Value')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    \n",
    "    # MinMax scaled\n",
    "    axes[2].hist(array_minmax[:, 0], bins=20, edgecolor='black', color='green')\n",
    "    axes[2].set_title('MinMax Scaled')\n",
    "    axes[2].set_xlabel('Value')\n",
    "    axes[2].set_ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb71a969",
   "metadata": {},
   "source": [
    "## Step 7: Using the Complete Pipeline\n",
    "\n",
    "The `excel_to_numpy` function performs all these steps automatically and saves the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b92de26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to: ./processed_data_output/excel_arrays.npz\n",
      "\n",
      "Arrays in archive: ['K-016084-677a::Sheet1', 'K-016084-677a::Sheet2', 'K-016084-677a::Sheet3']\n",
      "\n",
      "Metadata:\n",
      "{\n",
      "  \"K-016084-677a::Sheet1\": {\n",
      "    \"source_file\": \"/Users/aarijatiq/Documents/GitHub/Schneider_Arc_Flash_Prediction/Data/Processed Data/Extract Data 0.6kV Kinetrics/K-016084-677a.xls\",\n",
      "    \"sheet\": \"Sheet1\",\n",
      "    \"shape\": [\n",
      "      10005,\n",
      "      0\n",
      "    ],\n",
      "    \"columns\": [],\n",
      "    \"scale_mode\": \"standard\",\n",
      "    \"scale_params\": {},\n",
      "    \"dropna\": true,\n",
      "    \"fillna\": null,\n",
      "    \"numeric_only\": true\n",
      "  },\n",
      "  \"K-016084-677a::Sheet2\": {\n",
      "    \"source_file\": \"/Users/aarijatiq/Documents/GitHub/Schneider_Arc_Flash_Prediction/Data/Processed Data/Extract Data 0.6kV Kinetrics/K-016084-677a.xls\",\n",
      "    \"sheet\": \"Sheet2\",\n",
      "    \"shape\": [\n",
      "      0,\n",
      "      0\n",
      "    ],\n",
      "    \"columns\": [],\n",
      "    \"scale_mode\": \"standard\",\n",
      "    \"scale_params\": {},\n",
      "    \"dropna\": true,\n",
      "    \"fillna\": null,\n",
      "    \"numeric_only\": true\n",
      "  },\n",
      "  \"K-016084-677a::Sheet3\": {\n",
      "    \"source_file\": \"/Users/aarijatiq/Documents/GitHub/Schneider_Arc_Flash_Prediction/Data/Processed Data/Extract Data 0.6kV Kinetrics/K-016084-677a.xls\",\n",
      "    \"sheet\": \"Sheet3\",\n",
      "    \"shape\": [\n",
      "      0,\n",
      "      0\n",
      "    ],\n",
      "    \"columns\": [],\n",
      "    \"scale_mode\": \"standard\",\n",
      "    \"scale_params\": {},\n",
      "    \"dropna\": true,\n",
      "    \"fillna\": null,\n",
      "    \"numeric_only\": true\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Use the complete pipeline\n",
    "output_dir = \"./processed_data_output\"\n",
    "\n",
    "try:\n",
    "    archive_path = excel_to_numpy(\n",
    "        paths=[FILENAME],\n",
    "        sheet=None,  # Process all sheets\n",
    "        header=None,  # Use default header\n",
    "        skiprows=0,\n",
    "        columns=None,  # Use all columns\n",
    "        numeric_only=True,  # Keep only numeric columns\n",
    "        dropna=True,  # Drop rows with missing values\n",
    "        fillna=None,\n",
    "        scale=\"standard\",  # Apply standard scaling\n",
    "        output_dir=output_dir,\n",
    "        save_meta=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Data saved to: {archive_path}\")\n",
    "    \n",
    "    # Load the saved data\n",
    "    loaded_data = np.load(archive_path)\n",
    "    print(f\"\\nArrays in archive: {list(loaded_data.keys())}\")\n",
    "    \n",
    "    # Load metadata\n",
    "    meta_path = os.path.join(output_dir, \"excel_arrays.metadata.json\")\n",
    "    with open(meta_path, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    print(f\"\\nMetadata:\")\n",
    "    print(json.dumps(metadata, indent=2))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in pipeline: {e}\")\n",
    "    print(\"This is expected if FILENAME is not a valid file path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e879f464",
   "metadata": {},
   "source": [
    "## Step 8: Load Processed Data for ML Model\n",
    "\n",
    "Once the data is processed and saved, you can easily load it for ML model training or inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "959e198a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded processed data for ML:\n",
      "\n",
      "K-016084-677a::Sheet1:\n",
      "  Shape: (10005, 0)\n",
      "  Scaling: standard\n",
      "  Columns: []\n",
      "  Ready for ML model inference: Yes ✓\n",
      "\n",
      "K-016084-677a::Sheet2:\n",
      "  Shape: (0, 0)\n",
      "  Scaling: standard\n",
      "  Columns: []\n",
      "  Ready for ML model inference: Yes ✓\n",
      "\n",
      "K-016084-677a::Sheet3:\n",
      "  Shape: (0, 0)\n",
      "  Scaling: standard\n",
      "  Columns: []\n",
      "  Ready for ML model inference: Yes ✓\n"
     ]
    }
   ],
   "source": [
    "# Example: Load processed data for ML inference\n",
    "def load_processed_data_for_ml(archive_path, metadata_path):\n",
    "    \"\"\"\n",
    "    Load processed data ready for ML model inference.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary with array names as keys and processed arrays as values\n",
    "        dict: Metadata for each array\n",
    "    \"\"\"\n",
    "    # Load arrays\n",
    "    data = np.load(archive_path)\n",
    "    arrays = {key: data[key] for key in data.keys()}\n",
    "    \n",
    "    # Load metadata\n",
    "    with open(metadata_path, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    return arrays, metadata\n",
    "\n",
    "# Example usage\n",
    "if os.path.exists(output_dir):\n",
    "    try:\n",
    "        arrays, metadata = load_processed_data_for_ml(\n",
    "            os.path.join(output_dir, \"excel_arrays.npz\"),\n",
    "            os.path.join(output_dir, \"excel_arrays.metadata.json\")\n",
    "        )\n",
    "        \n",
    "        print(\"Loaded processed data for ML:\")\n",
    "        for key, arr in arrays.items():\n",
    "            print(f\"\\n{key}:\")\n",
    "            print(f\"  Shape: {arr.shape}\")\n",
    "            print(f\"  Scaling: {metadata[key]['scale_mode']}\")\n",
    "            print(f\"  Columns: {metadata[key]['columns']}\")\n",
    "            print(f\"  Ready for ML model inference: Yes ✓\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load data: {e}\")\n",
    "else:\n",
    "    print(f\"Output directory '{output_dir}' does not exist.\")\n",
    "    print(\"Run the pipeline with a valid Excel file first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7e4fce",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Reading Excel files** with multiple engine fallbacks for compatibility\n",
    "2. **Selecting columns** by name or index, filtering for numeric data\n",
    "3. **Handling missing values** through dropping or filling\n",
    "4. **Converting to NumPy arrays** for ML compatibility\n",
    "5. **Applying scaling** (standard or minmax) for feature normalization\n",
    "6. **Saving processed data** as .npz files with metadata\n",
    "7. **Loading data** for ML model training/inference\n",
    "\n",
    "### Command-line Usage\n",
    "\n",
    "You can also use the script from the command line:\n",
    "\n",
    "```bash\n",
    "# Process a single file\n",
    "python extract_files.py path/to/file.xlsx --numeric-only --dropna --scale standard\n",
    "\n",
    "# Process multiple files\n",
    "python extract_files.py file1.xlsx file2.xls --numeric-only --scale minmax --output-dir ./output\n",
    "\n",
    "# Process all files in a directory\n",
    "python extract_files.py ../Data/Processed\\ Data/ --numeric-only --dropna --scale standard\n",
    "```\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Replace `FILENAME` with actual Excel files from your dataset\n",
    "- Adjust scaling method based on your ML model requirements\n",
    "- Use the processed .npz files for model training or inference\n",
    "- Refer to the metadata JSON for information about the preprocessing applied"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alpha-beta-crown",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
